---
layout: default
---

# 嵌入式AI简报 (2020-11-30)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次19条。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 华为：新机将首发天玑700，入门级5G SoC，7nm工艺，8核CPU，2xA76@2.2GHz+6xA55@2.0GHz，集成Mail-G57 MC2 GPU，支持双模5G；  
- 华为：官宣出售荣耀。分拆的根本矛盾是华为芯片受到制约。华为打算在两年之内，实现20nm芯片的自主制造和量产，为此需千亿级别的投入，收购方是华为30多家合作伙伴成立的深圳市智信新信息技术有限公司，收购价2633亿。根据网络爆料，分拆后荣耀将三年内上市，原属于华为的Nova和畅想系列也会归入荣耀；  
- 联发科：将通过旗下子公司立锜并购Intel旗下Enpirion电源管理芯片产品线相关资产，预计交易金额约8500万美元，最快今年第四季度完成交易。原产品线针对FPGA芯片、车用处理器，以及低功耗平台的高集成度电源管理模块；  
- 台积电：2nm工艺突破，有望2023年下半年风险性试产，2024年步入量产。2nm上，台积电将放弃延续多年的FinFET(鳍式场效应晶体管)，甚至不用三星在3nm工艺上使用的GAAFET，也就是纳米线，而将其拓展成为“MBCFET“；  
- 苹果：2021年iPhone中的A15芯片将坚持使用5nm工艺，但会转向台积电称为“5nm+/N5P”的性能增强版。目前iPhone 12是使用台积电5nm制程的手机；  
- AMD：宣布350亿美元收购赛灵思，CPU、GPU、FPGA全凑齐，AMD希望借赛灵思的FPGA和SmartNIC等产品进一步占据数据中心市场；  
- 英伟达：发布2021财年第三季度财务报告，截至2020年10月25日第三季度收入创下47.3亿美元的纪录，较去年同期的30.1亿美元增长57%，较上一季度的38.7亿美元增长22%；  




> 注：个别链接打不开，请点击文末【阅读原文】跳转


## 业界新闻  


- [快手：将GPU推理在商业化场景全量落地，机器成本优化超20% | 机器之心](https://mp.weixin.qq.com/s/m7I9BguNWp5dGgOde5-yGA)  
摘要：对于推荐、广告等场景使用的大规模稀疏模型，业内一般用 TensorFlow 训练，GPU 场景推理选择 TensorFlow 或 TensorRT。对于 TensorFlow 和 TensorRT 的结合，常见的做法是将 TensorFlow 模型转到 ONNX 模型，然后从 ONNX 模型加载，这限制了模型的结构，也导致训练好的 TensorFlow 模型无法直接以端到端的形式应用于线上。   
快手借鉴业界经验，从实际业务出发，围绕大规模稀疏模型场景，针对发挥 GPU 算力和 TensorFlow 与 TensorRT 的结合易用性，进行了一系列技术上的探索和尝试。  
首先是计算流水优化，提升硬件利用率，利用多 cuda stream，同时运行多个 Compute Engine，增加 GPU 有效工作时间的占比，使每个 Compute Engine 对应两条 Cuda stream，优化了 H2D 数据传输到 GPU 计算的流水，自动对 TF graph 做裁剪，减少重复计算和内存拷贝，不断优化 CPU 到 GPU 的流水（比如对 user 侧 embedding 在卡上展开），达到算力均衡。此外，快手也结合场景和模型的特点，快手也进行了针对性的设计和优化，结合 CPU 和 GPU 的优势，保证大规模模型的线上应用。  
- [微软小冰：拿下首轮投资，开始「成年冒险」 | 极客公园](https://mp.weixin.qq.com/s/WxpjgXt7DJQ89QLHhM1HRA)  
摘要：今年 5 月，小冰科技有限公司注册成立。7 月中旬，小冰正式从微软拆分，成为一家独立运营的公司。沈向洋担任新公司董事长，李笛担任 CEO。新公司将继续保留中国小冰、日本 Rinna 品牌。至此，小冰脱离微软大体系，告别微软羽翼的庇护，从温室走向丛林。  
小冰从文本到多模态交互，覆盖、融合了 AI 人工智能领域几乎所有主流技术。支撑起全球超 60% 的 AI 交互总量，交互总量稳居全球第一。仅小冰单一品牌就覆盖了 6.6 亿在线用户、4.5 亿台第三方智能设备、9 亿内容用户。  
目前，小冰联合微软特别针对金融、智能汽车、内容生产三个领域场景推出行业解决方案。  
- [苹果：自研M1芯片登场，或将再次颠覆PC行业 | 三易生活](https://mp.weixin.qq.com/s/bBI95NbnG1GZOHjMCPkbiw)  
摘要：M1集成了“四大四小”的八个CPU核心，它们全都基于ARM指令集，但由苹果自行设计基础架构。并且值得一提的是，M1在CPU缓存的设计上表现得非常“慷慨”，比如说它的四枚性能核心共拥有192KB指令缓存、128KB数据缓存，以及12MB的共享二级缓存，这就相当于每一枚大核各自拥有48KB指令缓存、32KB数据缓存和3MB的二级缓存。而作为对比，当下PC领域的消费级旗舰处理器Intel Core i9-10900K，每个核心的缓存配置仅仅只有32KB指令、32KB数据、256KB二级缓存，以及2MB三级缓存。  
GPU部分其实也有着不少的看点。据悉，它同样基于苹果自研的图形处理器技术，采用了八核心的并行设计，并且共拥有128个执行单元。而值得注意的是，“执行单元”并不等于目前市场上PC端GPU的“流处理器”，毕竟一个执行单元内部是可能包含多个流处理器，因此在这一点上苹果与Intel的计算方式其实要更相似一些。它的像素填充率和纹理填充率分别是41GP/s和82GT/s，再加上高达2.6TFlops的浮点性能，这意味着M1的GPU性能超过了AMD锐龙4000系APU里的Vega8（1.792TFlops）和Intel十一代酷睿里的Xe 96EU核显（2.074TFlops），几乎已经达到了NVIDIA GeForce GTX1650Ti移动标压版（像素填充43.2GP/s、纹理填充86.4GT/s，浮点性能2.765TFlops）的水准。然而GeForce GTX1650Ti移动标压版是一款TDP高达55W的GPU，而M1的苹果自研图形处理器方案，则仅仅只是这款芯片内部的一小块“集显”而已。  

- [联发科：A78新主控解析，偷懒旗舰还是中端绝杀 | 三易生活](https://mp.weixin.qq.com/s/kBq7G-APJ2vPFKzlGWzjpg)  
摘要：2020年11月11日，联发科推出了他们新款的入门级5G SoC 天玑700，并且与此同时，他们还预告了一款采用6nm制程，配备主频高达3GHz的Cortex-A78新架构大核的全新“旗舰产品”。就在时隔数日后，这一新主控的GeekBench 4测试成绩就被曝光。  
新主控基于A78新架构的MT6893Z/CZA，3GHz主频、Mali-G77MC9 GPU方案，是一款面向大众消费者的普及型高端平台。  

- [快手：手机端分分钟拥有哈利波特的隐身衣 | 量子位](https://mp.weixin.qq.com/s/b69D_aYT9eKx0lD61T66Jw)  
摘要：要把画面中的人抹掉，除了自动把人像抠出来之外，AI还得学会脑补人像遮挡住的真实背景。这就涉及到两方面的问题：初始帧人像区域的背景修复，以及后续相机、人物运动过程中人像区域的背景填充。为了解决这两个问题，快手的工程师们将算法整体分成了两个阶段：首帧使用移动端脑补模型实现对人像区域的背景填充，后续帧使用帧间实时跟踪匹配投影，实现可见背景区域向人物遮挡区域的填充。
其中用到了基于开源的DeepFill图像修复算法。  
性能方面，则是使用自研的YCNN深度学习推理引擎。拿CPU来说，无论是苹果、高通、华为还是联发科的芯片，无论是高端的骁龙865还是低端的骁龙450、430，YCNN引擎都能支持模型在上面运行。同样，GPU方面，YCNN引擎同时支持Mali、Adreno、Apple和英伟达等多种GPU。NPU方面，苹果Bionic，华为HiAI，高通SNPE和MTK的APU均在支持范围之内。  

- [TensorFlow为新旧Mac特供新版本，GPU可用于训练，速度最高提升7倍 | 机器之心](https://mp.weixin.qq.com/s/hgVeOjs07y0rEu4gM9RNuw)  
摘要：TensorFlow 发文表示：我们专门做了一版为 Mac 用户优化的 TensorFlow 2.4 框架，M1 版 Mac 和英特尔版 Mac 都能用。这一举动有望大幅降低模型训练和部署的门槛。  
但新的 tensorflow_macos 分支利用苹果的 ML Compute，能让 GPU 也被利用起来。苹果在博客中介绍说：「我们使用了更高级别的优化方法，比如熔合层，选择合适的设备类型，将图作为原语编译、执行并由 CPU 上的 BNNS 和 GPU 上的 Metal Performance Shader 加速。」  
M1 芯片将苹果的神经网络引擎引入了 Mac，实现了 15 倍的机器学习任务加速。该神经网络引擎有 16 个核心，每秒运算速度可达 11 万亿次。除此之外，配置了 ML 加速器的 CPU 和强大 GPU（称霸集显，媲美部分独显）也使得整个 M1 芯片的机器学习能力得到巨大提升。Mac 版 TensorFlow 2.4 的详细入门指南可以参见：https://github.com/apple/tensorflow_macos  


## 论文

- [BERT轻量化：最优参数子集Bort，大小仅为BERT-large16% | 量子位](https://mp.weixin.qq.com/s/RlCAKi1UtRIiT8EYrpjUQA)  
摘要：亚马逊 Alexa 团队对BERT模型进行参数选择，获得了BERT的最优参数子集——Bort。Bort大小仅为BERT-large的16％，但是在CPU上的速度却快了7.9倍，在NLU基准测试上的性能也优于BERT-large。  
该研究通过完全多项式时间近似算法（FPTAS）优化改问题，因该算法最近被证明：在某些条件下，能够有效地提取此类最优子集。其中列举了三个指标：推理速度，参数大小和错误率，在使用FPTAS下，从一个高性能的BERT中提取一个最优子集，这便是Bort。  
- [基于Transformer的车道线检测算法：420 fps，又快又好](https://mp.weixin.qq.com/s/V2xmoUvzn8Ikvvf6BuZw-g)  
标题：[End-to-end Lane Shape Prediction with Transformers](https://arxiv.org/pdf/2011.04233.pdf)  
项目：https://github.com/liuruijin17/LSTR  
摘要：该文为车道线检测问题建立参数模型，使用Transformer捕获道路中细长车道线特征和全局特征，所发明的车道线检测算法与以往相比，可端到端训练、参数量更少、速度更快（高达420 fps，单1080Ti）。  
该文的一大目标即是将Transformer用于车道线检测，将其用于特征提取部分。此外，车道线检测以往的方法往往需要经过特征提取和后处理两个过程，这使得整个算法不能端到端训练，作者借助于对车道线曲线和相机内参的描述，采用多项式参数模型来描述车道线，并配以Bipartite Matching Loss函数，实现端到端训练，网络的目标成为预测几个参数，这无需后处理且降低了计算量。  
- [NeurIPS 2020 | MiniLM：通用模型压缩方法 | 小窗幽记机器学习](https://mp.weixin.qq.com/s/iLO1FOE-4z1p07RCfCJIaA)  
摘要：预训练模型过大的话，有2个弊端：推理速度慢，内存空间占用大。为解决该问题，作者提出了一种通用的面向Transformer-based预训练模型压缩方法：MiniLM。MiniLM有3个核心点：1. 蒸馏teacher模型最后一层Transformer的自注意力模块；2. 在自注意模块中引入值之间的点积；3. 引入助教模型辅助模型蒸馏。  
实验表明，各种参数尺寸的student模型中，MiniLM的单语种模型优于各种最先进的蒸馏方案。在 SQuAD 2.0和GLUE的多个任务上以一半的参数和计算量就保持住99%的accuracy。此外，MiniLM在多语种预训练模型上也取得不错的结果。  


## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [tensorflow/tensorflow: TensorFlow 2.4.0 候选版本发布 | TensorFlow](https://mp.weixin.qq.com/s/Mf4N_F5xRUuB0VklK7wjLg)  
摘要：这里仅说一下TFLite方面的主要特性，TFLite Profiler Android 版本现已推出，tf.lite引入quantization.quantize_and_dequantize_v2，可以更新超过范围的量化的梯度定义，更多信息参考：https://github.com/tensorflow/tensorflow/releases/tag/v2.4.0-rc0 ；
- [OAID/AutoKernel: 一个简单易用，低门槛的自动算子优化工具，提高深度学习算法部署效率](https://github.com/OAID/AutoKernel)  
摘要：AutoKernel是由OPEN AI LAB提出的高性能算子自动优化工具，可以自动优化调度策略、生成底层优化代码，大幅减少各硬件芯片算子开发成本，提升算子优化效率，让工程师更快实现深度学习算法在各硬件芯片上的高性能部署。AutoKernel特色：低门槛、简单易用、高效率。AutoKernel分为三个模块：算子生成器，自动搜索模块，以及算子部署插件。  
算子生成器：该模块使用了开源项目Halide；Halide是业界广泛使用的自动代码生成项目，它首次提出将计算和调度分离。该模块的输入是和硬件无关的算子计算描述，输出是相应后端的优化汇编代码/目标文件；  
自动搜索模块：该模块可以通过最优化算法/搜索算法/机器学习/强化学习搜索出相应后端的最优算子的调度策略参数（该模块仍在开发中）；  
算子部署插件（ AutoKernel Plugin）：Tengine是OPEN AILAB开源的深度学习推理框架，实现了AI算法在不同硬件的快速高效部署。该模块实现了将自动生成的优化算子代码以plugin的形式一键集成到Tengine中，实现自动优化算子的一键部署。  
- [RangiLyu/nanodet: NanoDet轻量级（1.8MB）、超快速（移动端97fps）目标检测项目 | OpenCV中文网](https://mp.weixin.qq.com/s/KC-QxYZf2471OICDFra7Zw)  
代码：https://github.com/RangiLyu/nanodet/
摘要：目标检测模型NanoDet特点如下：超轻量级：模型文件只有1.8 MB；超快速：在移动 ARM CPU 上达到 97fps(10.23ms)；训练友好：训练占用内存少。可设置Batch-size=80，在 GTX1060 6G上也可以训练；易部署：作者提供了基于 ncnn 推理的 C++ 实现和安卓部署 demo。  


## 博文

- [基于JS语言的DL框架——7个浏览器深度学习框架调研：还有很长的路要走 | 机器之心](https://mp.weixin.qq.com/s/C7QdVathJ8YTXF-zXPC-Ow)  
摘要：
作者基于WWW’19 论文提供的线索，详细解读了在浏览器中实现深度学习的可能性、可行性和性能现状。具体而言，作者重点分析了 7 个最近出现的基于JavaScript 的 DL 框架，并对比了具体框架支持哪些 DL 任务。其中包括：TensorFlow.js、ConvNetJS、Keras.js、WebDNN、brain.js、synaptic、Mind。除性能外，作者还比较了模型大小，支持的GPU后端数量，模型体积大小等。


> 注：个别链接打不开，请点击文末【阅读原文】跳转


## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  |  | [2020-10-21](../embedded-ai-report/2020-10-21.md) | [2020-09-17](../embedded-ai-report/2020-09-17.md) |
| [2020-08-26](../embedded-ai-report/2020-08-26.md) | [2020-08-06](../embedded-ai-report/2020-08-06.md) | [2020-07-18](../embedded-ai-report/2020-07-18.md) | [2020-07-02](../embedded-ai-report/2020-07-02.md) |
| [2020-06-17](../embedded-ai-report/2020-06-17.md) | [2020-06-03](../embedded-ai-report/2020-06-03.md)  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
