---
layout: default
---

# 嵌入式AI简报 (2020-11-30)

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次19条。「新闻」华为麒麟9000处理器SoC，ARM中国发布周易Z2 AIPU单核算力4TOPS，ARM开发者峰会揭晓未来两代Cortex CPU架构，三星打造5nm Exynos1080高端SoC，谷歌Coral Mini开始预定，Graphcore发布Poplar SDK 1.3；「论文」比EfficientNet快3.5倍的LambdaResNets实现视觉任务新SOTA，针对Transform极低位数的混合精度量化策略，对手机GPU设备做推理耗时预估的LETI方法，软硬件人工智能解决方案的协同设计方法讨论；「开源」OpenCV4.5发布:端上推理性能提升到第一梯队，谷歌开源与BERT精度相同参数更少的新模型pQRNN，TensorFlow JS后端强化性能再提升，国内训练框架MindSpore 1.0发布，OneFlow v0.2.0发布；「博文」模糊边缘与数据中心的高通Cloud AI 100芯片解析，不同NVIDIA GPU计算能力如何评估，TensorFlow Lite开源设备端推荐解决方案，详解卷积中的Winograd加速算法，OpenCV4.4 CUDA编译与加速全解析。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 华为：麒麟9000处理器，预计是首款5G 5nm SoC，四核心Cortex-A77+四核心Cortex-A55的设计，最高主频达到了3.13GHz。Mali-G78 24个核心堆到了G78所支持的上限；  
- Imagination：宣布推出全新IMG B系列GPU IP。其第一个包含新多核架构的GPU IP系列，峰值算力达6 TFLOPS，AI算力可达24 TOPS，其汽车GPU已符合ISO 26262安全标准要求，并支持自动驾驶和辅助驾驶；  
- 高通：骁龙875曝光，5nm EUV，代号为SM8350，内部称为Lahaina（地名，夏威夷群岛），有plus版。采用“1+3+4”八核心设计，首次采用Cortex X1超大核+Cortex A78大核这样的组合，其中Cortex X1的峰值性能比Cortex A78高23%，堪称真正意义上的“超大核”；  
- 高通：骁龙750G 5G发布，定位中端，代号为SM7225，8nm，CPU为8核心设计，由两个频率为2.2GHz的A77大核和六个1.8GHz的A55组成，GPU集成Adreno 619，商用版2020年底上市；  
- 高通：骁龙775G曝光，6nm，定位次旗舰，代号为SM7350，内部称为Cedros；  
- 树莓派：树莓派4计算模组上线，25美元起。树莓派4的计算模组主要位置：1.5GHz 4核ACortex-A72 CPU，VideoCore VI 显卡，4k@60帧硬件解码，支持H.265(HEVC)和1080p@60帧硬件解码，以及 1080p@30帧硬件解码，支持H.264(AVC)等。  



> 注：个别链接打不开，请点击文末【阅读原文】跳转


## 业界新闻  


- [快手将GPU推理在商业化场景全量落地，机器成本优化超20% | 机器之心](https://mp.weixin.qq.com/s/m7I9BguNWp5dGgOde5-yGA)  
摘要：对于推荐、广告等场景使用的大规模稀疏模型，业内一般用 TensorFlow 训练，GPU 场景推理选择 TensorFlow 或 TensorRT。对于 TensorFlow 和 TensorRT 的结合，常见的做法是将 TensorFlow 模型转到 ONNX 模型，然后从 ONNX 模型加载，这限制了模型的结构，也导致训练好的 TensorFlow 模型无法直接以端到端的形式应用于线上。   
快手借鉴业界经验，从实际业务出发，围绕大规模稀疏模型场景，针对发挥 GPU 算力和 TensorFlow 与 TensorRT 的结合易用性，进行了一系列技术上的探索和尝试。  
首先是计算流水优化，提升硬件利用率，利用多 cuda stream，同时运行多个 Compute Engine，增加 GPU 有效工作时间的占比，使每个 Compute Engine 对应两条 Cuda stream，优化了 H2D 数据传输到 GPU 计算的流水，自动对 TF graph 做裁剪，减少重复计算和内存拷贝，不断优化 CPU 到 GPU 的流水（比如对 user 侧 embedding 在卡上展开），达到算力均衡。此外，快手也结合场景和模型的特点，快手也进行了针对性的设计和优化，结合 CPU 和 GPU 的优势，保证大规模模型的线上应用。  
- [拿下首轮投资，微软小冰开始「成年冒险」 | 极客公园](https://mp.weixin.qq.com/s/WxpjgXt7DJQ89QLHhM1HRA)  
摘要：今年 5 月，小冰科技有限公司注册成立。7 月中旬，小冰正式从微软拆分，成为一家独立运营的公司。沈向洋担任新公司董事长，李笛担任 CEO。新公司将继续保留中国小冰、日本 Rinna 品牌。至此，小冰脱离微软大体系，告别微软羽翼的庇护，从温室走向丛林。  
小冰从文本到多模态交互，覆盖、融合了 AI 人工智能领域几乎所有主流技术。支撑起全球超 60% 的 AI 交互总量，交互总量稳居全球第一。仅小冰单一品牌就覆盖了 6.6 亿在线用户、4.5 亿台第三方智能设备、9 亿内容用户。  
目前，小冰联合微软特别针对金融、智能汽车、内容生产三个领域场景推出行业解决方案。  
- [自研M1芯片登场，苹果或将再次颠覆PC行业 | 三易生活](https://mp.weixin.qq.com/s/bBI95NbnG1GZOHjMCPkbiw)  
摘要：M1集成了“四大四小”的八个CPU核心，它们全都基于ARM指令集，但由苹果自行设计基础架构。并且值得一提的是，M1在CPU缓存的设计上表现得非常“慷慨”，比如说它的四枚性能核心共拥有192KB指令缓存、128KB数据缓存，以及12MB的共享二级缓存，这就相当于每一枚大核各自拥有48KB指令缓存、32KB数据缓存和3MB的二级缓存。而作为对比，当下PC领域的消费级旗舰处理器Intel Core i9-10900K，每个核心的缓存配置仅仅只有32KB指令、32KB数据、256KB二级缓存，以及2MB三级缓存。  
GPU部分其实也有着不少的看点。据悉，它同样基于苹果自研的图形处理器技术，采用了八核心的并行设计，并且共拥有128个执行单元。而值得注意的是，“执行单元”并不等于目前市场上PC端GPU的“流处理器”，毕竟一个执行单元内部是可能包含多个流处理器，因此在这一点上苹果与Intel的计算方式其实要更相似一些。它的像素填充率和纹理填充率分别是41GP/s和82GT/s，再加上高达2.6TFlops的浮点性能，这意味着M1的GPU性能超过了AMD锐龙4000系APU里的Vega8（1.792TFlops）和Intel十一代酷睿里的Xe 96EU核显（2.074TFlops），几乎已经达到了NVIDIA GeForce GTX1650Ti移动标压版（像素填充43.2GP/s、纹理填充86.4GT/s，浮点性能2.765TFlops）的水准。然而GeForce GTX1650Ti移动标压版是一款TDP高达55W的GPU，而M1的苹果自研图形处理器方案，则仅仅只是这款芯片内部的一小块“集显”而已。  





## 论文

- [BERT轻量化：最优参数子集Bort，大小仅为BERT-large16% | 量子位](https://mp.weixin.qq.com/s/RlCAKi1UtRIiT8EYrpjUQA)  
摘要：亚马逊 Alexa 团队对BERT模型进行参数选择，获得了BERT的最优参数子集——Bort。Bort大小仅为BERT-large的16％，但是在CPU上的速度却快了7.9倍，在NLU基准测试上的性能也优于BERT-large。  
该研究通过完全多项式时间近似算法（FPTAS）优化改问题，因该算法最近被证明：在某些条件下，能够有效地提取此类最优子集。其中列举了三个指标：推理速度，参数大小和错误率，在使用FPTAS下，从一个高性能的BERT中提取一个最优子集，这便是Bort。  
- [基于Transformer的车道线检测算法：420 fps，又快又好](https://mp.weixin.qq.com/s/V2xmoUvzn8Ikvvf6BuZw-g)  
标题：[End-to-end Lane Shape Prediction with Transformers](https://arxiv.org/pdf/2011.04233.pdf)  
项目：https://github.com/liuruijin17/LSTR  
摘要：该文为车道线检测问题建立参数模型，使用Transformer捕获道路中细长车道线特征和全局特征，所发明的车道线检测算法与以往相比，可端到端训练、参数量更少、速度更快（高达420 fps，单1080Ti）。  
该文的一大目标即是将Transformer用于车道线检测，将其用于特征提取部分。此外，车道线检测以往的方法往往需要经过特征提取和后处理两个过程，这使得整个算法不能端到端训练，作者借助于对车道线曲线和相机内参的描述，采用多项式参数模型来描述车道线，并配以Bipartite Matching Loss函数，实现端到端训练，网络的目标成为预测几个参数，这无需后处理且降低了计算量。  




## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [OAID/AutoKernel: ](https://github.com/OAID/AutoKernel)  
摘要：AutoKernel是由OPEN AI LAB提出的高性能算子自动优化工具，可以自动优化调度策略、生成底层优化代码，大幅减少各硬件芯片算子开发成本，提升算子优化效率，让工程师更快实现深度学习算法在各硬件芯片上的高性能部署。AutoKernel特色：低门槛、简单易用、高效率。AutoKernel分为三个模块：算子生成器，自动搜索模块，以及算子部署插件。  
算子生成器：该模块使用了开源项目Halide；Halide是业界广泛使用的自动代码生成项目，它首次提出将计算和调度分离。该模块的输入是和硬件无关的算子计算描述，输出是相应后端的优化汇编代码/目标文件；  
自动搜索模块：该模块可以通过最优化算法/搜索算法/机器学习/强化学习搜索出相应后端的最优算子的调度策略参数（该模块仍在开发中）；  
算子部署插件（ AutoKernel Plugin）：Tengine是OPEN AILAB开源的深度学习推理框架，实现了AI算法在不同硬件的快速高效部署。该模块实现了将自动生成的优化算子代码以plugin的形式一键集成到Tengine中，实现自动优化算子的一键部署。  


## 博文

- [基于JS语言的DL框架——7个浏览器深度学习框架调研：还有很长的路要走 | 机器之心](https://mp.weixin.qq.com/s/C7QdVathJ8YTXF-zXPC-Ow)  
摘要：
作者基于WWW’19 论文提供的线索，详细解读了在浏览器中实现深度学习的可能性、可行性和性能现状。具体而言，作者重点分析了 7 个最近出现的基于JavaScript 的 DL 框架，并对比了具体框架支持哪些 DL 任务。其中包括：TensorFlow.js、ConvNetJS、Keras.js、WebDNN、brain.js、synaptic、Mind。除性能外，作者还比较了模型大小，支持的GPU后端数量，模型体积大小等。


> 注：个别链接打不开，请点击文末【阅读原文】跳转


## [往期回顾](https://github.com/ysh329/awesome-embedded-ai)


| 2 | 0 | 2 | 0 |
|:---:|:---:|:---:|:---:|
|  |  | [2020-10-21](../embedded-ai-report/2020-10-21.md) | [2020-09-17](../embedded-ai-report/2020-09-17.md) |
| [2020-08-26](../embedded-ai-report/2020-08-26.md) | [2020-08-06](../embedded-ai-report/2020-08-06.md) | [2020-07-18](../embedded-ai-report/2020-07-18.md) | [2020-07-02](../embedded-ai-report/2020-07-02.md) |
| [2020-06-17](../embedded-ai-report/2020-06-17.md) | [2020-06-03](../embedded-ai-report/2020-06-03.md)  | [2020-05-15](../embedded-ai-report/2020-05-15.md) | [2020-04-26](../embedded-ai-report/2020-04-26.md) |  
| [2020-04-04](../embedded-ai-report/2020-04-04.md) | [2020-03-19](../embedded-ai-report/2020-03-19.md) | [2020-03-02](../embedded-ai-report/2020-03-02.md) | [2020-02-16](../embedded-ai-report/2020-02-16.md) |  
| [2020-01-27](../embedded-ai-report/2020-01-27.md) | [2020-01-06](../embedded-ai-report/2020-01-06.md) | [2019-12-17](../embedded-ai-report/2019-12-17.md)  |  [2019-12-02](../embedded-ai-report/2019-12-02.md) |
| 2 | 0 | 1 | 9 |  
| [2019-11-30](../embedded-ai-report/2019-11-30.md) | [2019-11-18](../embedded-ai-report/2019-11-18.md) | [2019-10-31](../embedded-ai-report/2019-10-31.md)  |  [2019-10-17](../embedded-ai-report/2019-10-17.md) |  
| [2019-10-03](../embedded-ai-report/2019-10-03.md) | [2019-09-16](../embedded-ai-report/2019-09-16.md) | [2019-08-30](../embedded-ai-report/2019-08-30.md)  |  [2019-08-15](../embedded-ai-report/2019-08-15.md) |  
| [2019-07-30](../embedded-ai-report/2019-07-30.md) | [2019-07-15](../embedded-ai-report/2019-07-15.md) | [2019-06-29](../embedded-ai-report/2019-06-29.md)  |  [2019-06-17](../embedded-ai-report/2019-06-17.md) |  
| [2019-05-30](../embedded-ai-report/2019-05-30.md) | [2019-05-15](../embedded-ai-report/2019-05-15.md) | [2019-04-27](../embedded-ai-report/2019-04-27.md)  |  [2019-04-13](../embedded-ai-report/2019-04-13.md) |  
| [2019-03-31](../embedded-ai-report/2019-03-31.md) | | |  

----

![wechat_qrcode](../wechat_qrcode.jpg)

> 往期回顾：见公众号主菜单【历史消息】
- WeChat: NeuralTalk  
- Editor: https://github.com/ysh329  
- Project: https://github.com/ysh329/awesome-embedded-ai  

----

<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 通用许可协议</a>进行许可。
