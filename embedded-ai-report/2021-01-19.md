---
layout: default
---

# 嵌入式AI简报 (2021-01-19)  

**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：本次17条。【新闻】；【论文】；【开源】；【博文】。


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 三星：发布5nm全新旗舰SoC——Exynos 2100，S21将首发：CPU为Cortex-1xX1@2.9GHz+3xA78@2.8GHz+4xA55@2.2GHz，GPU为Mali-G78 MP14，14个核心其支持AMIGO多IP调控技术，可以联合优化CPU、GPU及其他部分的功耗，高负载下也能延长续航时间。NPU方面，这次Exynos 2100拥有3个AI核心，性能可达26TOPS，也就是26万次操作性能。ISP方面，Exynos 2100单路最高可以支持2亿像素，支持6个图像传感器，并同时处理4个以提升多摄体验；
- 光子芯片：Nature连发两篇光子AI芯片论文，《用于光学神经网络的11 TOPS光子卷积加速器（11 TOPS photonic convolutional accelerator for optical neural networks）》、《利用积分光子张量核的并行卷积处理（Parallel convolutional processing using an integrated photonic tensor core）》；
- 高通：与长城汽车合作，后者将基于高通Ride平台打造自己的自动驾驶系统Coffee Intelligence； 
- 联发科：成为 MLCommons 创始成员携手联盟成员推动人工智能标准。MLCommons 是一个开放式 AI 创新实践产业联盟，由多家全球领导厂商发起成立（微软/facebook/alibaba/baidu等），将共同致力于推进机器学习和人工智能的标准及衡量指标；
- 谷歌 & 高通：双方将合作增强并扩展Project Treble项目，从刚刚发布的骁龙888开始，全部骁龙平台将支持四个Android操作系统版本与四年的安全更新。手机厂商无需调整相关软件，即可将骁龙移动平台的终端升级至最新的Android版本；
- 旷视：继云从、依图后，旷视冲击科创板，就剩商汤；
- Ambarella：CES 2021发布5nm SoC-CV5，支持 8K 60fps 视频录制或四路独立的 4K 视频流，极高能效比将8K@30fps视频录制能耗控制在 2W 以下，8K@60fps录制能耗控制在 5W 以下；  
- 高通：14亿美元买下芯片初创公司Nuvia，目的减轻高通对 Arm Ltd 的依赖。高通目前大多芯片从Arm获得许可的计算内核，而Nuvia的内核使用Arm的基础体系结构，但都是定制设计。对于高通公司来说，使用更多的自定义核心设计（苹果公司也已采取了这种做法）可以在短期内降低 Arm 的部分许可成本，从长远来看，可以更轻松地过渡到竞争对手的架构；
- 高通：推第二代超声波指纹识别器3D Sonic Sensor Gen 2，新版为传感器提供了更大的表面积和更快的处理速度，从而能更快地解锁手机比初代快50%。


> 注：个别链接打不开，请点击文末【阅读原文】跳转


## 业界新闻  

- [Graphcore创新社区共建：与阿里云HALO及微软亚洲研究院NNFusion | Graphcore](https://mp.weixin.qq.com/s/tJBpXRcALG24KM7MJDSsQw)  
摘要：Graphcore现已携手阿里云在GitHub上开源了专为阿里云HALO定制的代码odla_PopArt，意味着从GitHub上下载HALO可以直接在IPU上适配。Graphcore是阿里云HALO/ODLA的合作伙伴之一。早在2020年5月，阿里云就在OCP全球峰会上首次对外公布了ODLA接口标准，并宣布已率先在Graphcore等生态伙伴上获得支持[github.com/alibaba/heterogeneity-aware-lowering-and-optimization#halo](https://github.com/alibaba/heterogeneity-aware-lowering-and-optimization#halo)。  
微软亚洲研究院打造的深度神经网络编译器NNFusion使用统一的接口，让模型能够在不同的硬件厂商的芯片上无缝的运行。研究表明，在IPU上，LSTM的训练模型相比英伟达和AMD的GPU以及TPU，得到了3倍的提升。NNFusion现已在GitHub开源[github.com/microsoft/nnfusion](https://github.com/microsoft/nnfusion)。  

- [在移动设备上实现实时LiDAR 3D目标检测 | 
 
3D目标检测是一项重要任务，尤其是在自动驾驶应用领域。然而，在自动驾驶汽车的边缘计算设备上以有限的计算和内存资源来支持实时性能具有挑战性。为了实现这一目标，我们提出了一个具有编译器感知能力的统一框架，该框架将网络增强和剪枝搜索与强化学习技术结合在一起，以便能够在资源受限的边缘计算设备上实时推断3D目标检测。具体而言，使用生成器循环神经网络（RNN）来提供统一的方案，以自动进行网络增强和剪枝搜索，而无需人工和专业知识。统一方案的评估性能可以反馈给训练生成器RNN。实验结果表明，该框架首先在具有竞争优势的移动设备（三星Galaxy S20手机）上实现了实时3D目标检测
 
东北大学


## 论文

- [神经网络架构搜索介绍 | 专知](https://mp.weixin.qq.com/s/ntFyLBSHJGGUfK6y7wvIiw)  
标题：Neural Architecture Search: Comming of Age | University of Freiburg & Bosch Center for AI  
摘要：本文讨论围绕NAS建立科学社区的各种工作，包括基准测试、最佳实践和开放源码框架。之后讨论了该领域几个令人兴奋的方向:（1）广泛的NAS加速技术;（2）在Auto-PyTorch中结合NAS +超参数优化，实现现成的AutoML;（3）神经集成搜索（NES）的扩展问题定义，它搜索一组互补的架构，而不是像NAS中搜索的单一架构。  



## 开源项目


- [Tencent/TNN: 行业首发Arm 32位 FP16指令加速，理论性能翻倍](https://mp.weixin.qq.com/s?__biz=MjM5NDczOTA4NQ==&mid=2447885496&idx=1&sn=28cd45019ef474f53001a146f2ceec2d&chksm=b290425a85e7cb4c611eed166be8f8b6a7d594ee6946f7c2e90205bc62cabc60be1a5c1359bc&token=1368417534&lang=zh_CN#rd)  
摘要：行业开源推理框架如ncnn、MNN等仅支持Arm64位FP16指令加速，这样32位App无法享受FP16指令加速效果。针对这个行业缺失，TNN在架构兼容、模型兼容、代码结构设计等方面率先进行探索，对Arm64位和Arm32架构均实现了FP16指令优化，让64位和32位App都能发挥硬件FP16向量加速的能力。  
- [华为智慧终端背后的黑科技----超轻量AI引擎MindSpore Lite | MindSpore](https://mp.weixin.qq.com/s/D2uWrUJBIK8s0vsH7TVN1A)  
摘要：MindSpore Lite是MindSpore全场景AI框架的端侧引擎，目前MindSpore Lite作为华为HMS Core机器学习服务的推理引擎底座，已为全球1000+应用提供推理引擎服务，日均调用量超过3亿，同时在各类手机、穿戴感知、智慧屏等设备的AI特性上得到了广泛应用。  
MindSpore Lite 1.0.0 已经在今年9月份开源，开源之后，其接口易用性、算子性能与完备度、第三方模型的广泛支持等方面，得到了众多手机应用开发者的广泛认可。目前，1.0.0版本发布已经过去3个月，最新的MindSpore Lite 1.1.0 在算子性能优化、模型小型化、加速库自动裁剪工具、端侧模型训练、语音类模型支持、Java接口开放、模型可视化等方面进行了全面升级，升级后的版本更轻、更快、更易用。接下来我们看一下 MindSpore Lite 1.1.0 。获取1.1.0版本MindSpore Lite：https://www.mindspore.cn/tutorial/lite/zh-CN/r1.1/use/downloads.html  
推理性能优化依然是我们本次版本的重头戏，除了持续的ARM CPU(FP16/FP32/INT8)性能优化，ARM GPU和X86_64的优化也是本次的亮点，GPU方面我们除了传统的算子优化，还加入了在线融合、AutoTuning等技术，使得ARM GPU推理性能大幅提升；同时为了更好的支持PC侧推理，在X86_64算子方面我们做了大量汇编层面的优化；经过大量模型的实测，MindSpore Lite 1.1.0 在推理性能方面在业界各类框架中极具竞争力；  
端侧训练框架带来了以下特性：支持30+反向算子，提供SGD、ADAM等常见优化器及CrossEntropy/SparsCrossEntropy/MSE等损失函数；既可从零训练模型，也可指定特定网络层微调，达到迁移学习目的；支持LeNet/AlexNet/ResNet/MobileNetV1/V2/V3和EffectiveNet等网络训练，提供完整的模型加载，转换和训练脚本，方便用户使用和调测;云侧训练和端侧训练实现无缝对接，云侧模型可直接加载到端侧进行训练；支持checkpoint机制，训练过程异常中断后可快速恢复继续训练；华为部分设备的AI应用比如家庭相册等场景进行了商用，并取得了很好的用户体验。  
- [https://github.com/margaretmz/awesome-tensorflow-lite]()  
摘要：有很多例子。  

- [https://github.com/ml-gde/e2e-tflite-tutorials](https://github.com/ml-gde/e2e-tflite-tutorials)  
摘要：这个repo也可以参考一下。也欢迎大家分享看到的新例子



- [VainF/Torch-Pruning: A pytorch pruning toolkit for structured neural network pruning and layer dependency maintaining.](https://github.com/VainF/Torch-Pruning)  
摘要：This tool will automatically detect and handle layer dependencies (channel consistency) during pruning. It is able to handle various network architectures such as DenseNet, ResNet, and Inception. See examples/test_models.py for more supported models.  
- [sovrasov/flops-counter.pytorch: Flops counter for convolutional networks in pytorch framework](https://github.com/sovrasov/flops-counter.pytorch)  
摘要：This script is designed to compute the theoretical amount of multiply-add operations in convolutional neural networks. It also can compute the number of parameters and print per-layer computational cost of a given network.  
Supported layers: Conv1d/2d/3d (including grouping), ConvTranspose1d/2d/3d (including grouping), BatchNorm1d/2d/3d, Activations (ReLU, PReLU, ELU, ReLU6, LeakyReLU), Linear, Upsample
Poolings (AvgPool1d/2d/3d, MaxPool1d/2d/3d and adaptive ones).  
Experimental support: RNN, LSTM, GRU (NLH layout is assumed), RNNCell, LSTMCell, GRUCell.  
- [tencent-ailab/pika: a lightweight speech processing toolkit based on Pytorch and (Py)Kaldi](https://github.com/tencent-ailab/pika)  
摘要：PIKA is a lightweight speech processing toolkit based on Pytorch and (Py)Kaldi. The first release focuses on end-to-end speech recognition. We use Pytorch as deep learning engine, Kaldi for data formatting and feature extraction.  
Key Features: 1. On-the-fly data augmentation and feature extraction loader; 2. TDNN Transformer encoder and convolution and transformer based decoder model structure; 3. RNNT training and batch decoding; 4. RNNT decoding with external Ngram FSTs (on-the-fly rescoring, aka, shallow fusion); 5. RNNT Minimum Bayes Risk (MBR) training; 6. LAS forward and backward rescorer for RNNT; 7. Efficient BMUF (Block model update filtering) based distributed training.  

## 博文

- [Paddle Serving全新设计Pipeline Serving！带来更高吞吐量、更高GPU利用率 | 飞桨PaddlePaddle](https://mp.weixin.qq.com/s/ccTiNqcz62n3ANwc_ZLEZg)  
摘要：Paddle Serving是飞桨服务化部署框架，其0.4.0版本有如下三点核心升级：  
    1. 支持Pipeline Serving全异步设计的推理服务部署框架，实验证明，OCR任务的吞吐量提升1.5倍，GPU利用率提升1倍；  
    2. 支持NVIDIA TensorRT高性能推理库，实现模型推理低延迟、高吞吐；  
    3. 支持多系统、多语言客户端，覆盖更多的部署场景；  
