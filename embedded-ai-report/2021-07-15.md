---
layout: default
---

# 嵌入式AI简报 (2021-07-15)：


**关注模型压缩、低比特量化、移动端推理加速优化、部署**  

> 导读：


好了，先是一些热身小新闻ヽ(✿゜▽゜)ノ：

- 高通：受不了三星制程性能太差，骁龙895+ 或将采用台积电 4 纳米，高通预计在2022 年底推出的骁龙Snapdragon 895+ 移动处理将会采用苹果A16 Bionic 处理器相同的台积电4 纳米制程来生产；
- 格林深瞳：科创板IPO获上交所受理，本次拟募资10亿元。较之「AI四小龙」，格灵深瞳已然不在第一阵营，2B让收入起来但现金回不来，ZF生意难做。AI公司还没有强到可打破既定行业规则；
- 中移芯片 OneChip ：中国移动旗下，中移物联网全资子公司芯昇科技有限公司正式独立运行，进军物联网芯片领域并计划科创板上市；
- nVIDIA：为顺利收购Arm，英伟达对英国学术及医疗企业开放全英最快超级电脑剑桥-1，NVIDIA 还表示将使用Arm IP来设计芯片，并将在英国建立一个超算中心；
- CINNO Research：《中国手机通信产业数据观察报告》5月手机芯片出货量表示，海思跌落，紫光展锐暴涨 63 倍以 80 万片的当月手机芯片出货量跻身前五，同比增长了 6346.2%。，联发科成最大赢家；

> 注：个别链接打不开，请点击文末【阅读原文】跳转。


## 业界新闻  

- [上海张江又一款AI芯发布！性能超英伟达T4，AMD背景团队创办，快手投资 | 量子位](https://mp.weixin.qq.com/s/ufZ1M2B819_O_JHSIzQhfg)  
摘要：今年4月刚刚5亿元人民币A+轮融资的AI芯片公司瀚博半导体，发布了云端推理AI芯片SV100系列，以及搭载该系列芯片的AI推理加速卡VA1。在发布会上，瀚博半导体创始人兼CEO钱军展示了SV100系列的首款产品——SV102智能视觉芯片，瀚博半导体CTO张磊展示了VA1板卡。瀚博半导体这两款产品主要瞄准了AI领域中成熟的CV市场，主打低延时、多路视频处理能力，相比市面上的GPU在能效上具有优势，可节省60%服务器成本。  
瀚博半导体CEO和CTO均来自AMD，公司于2018年2月创立，作为一家新兴AI芯片公司，瀚博发展迅速，在去年5月就完成了首颗半定制7nm芯片流片，之后又获得了快手等领投的5000万美元的A轮融资。
云端推理芯片SV100系列是瀚博为云端推理而打造的服务器芯片，SV102是该系列的第一款芯片，INT8峰值算力达到200TOPS。SV102支持64路1080p视频解码，最大功耗为75W，接口为PCIe Gen4 x16，采用被动散热。这款芯片在今年6月刚刚测试成功。  
搭载SV102的VA1采用单宽半高半长75瓦PCIe卡设计，因此相比一般的GPU卡更节省能耗与空间。张磊表示，在ResNet-50的基准测试中，VA1是英伟达T4吞吐量的2倍以上。由于VA1节省服务器50%以上TCO，因此在一个2U服务器中，搭载VA1的设备可以解码384路视频，整机算力是T4设备的2.5倍以上，功耗比相同搭载GPU的服务器更低，可节省60%的服务器成本。在视频处理方面，VA1支持64路以上H264、H265或AVS2 1080p解码，分辨率支持高达8K。  
SV102芯片与VA1板卡预计将于今年第四季度量产。除了硬件产品外，瀚博也搭建了自己的VastStream AI软件平台，支持PyTorch、TensorFlow等主流AI框架，目前正在与CentOS、Ubuntu、Red Hat、银河麒麟等服务器操作系统进行适配。  
- [燧原发布中国最大AI芯片，顺手拿下四个国内第一，带Benchmark的那种 | 量子位](https://mp.weixin.qq.com/s/vhmaL4fdGTFApfBYQ9RwWQ)  
摘要：邃思2.0的到来，燧原科技其他产品也得到了相应地升级。首先，是云燧T20 训练加速卡。它是面向数据中心的第二代AI训练加速卡，官方介绍是这样的：具有模型覆盖面广、性能强、软件生态开放等特点，可支持多种人工智能训练场景。  云燧T21单精度FP32算力最高可达40TFLOPS、TF32算力最高则是160TFLOPS。  
最后，燧原科技还升级了它家的驭算TopsRider软件平台：基于算子泛化技术及图优化策略，支持主流深度学习框架下的各类模型训练。利用Horovod分布式训练框架与GCU-LARE互联技术相互配合，为超大规模集群的高效运行提供解决方案。而且编程模型和可扩展算子接口，都是开放的。  
最后，还给出了其产品的超摩尔定律：即燧原的每一代产品必须比前一代在“平均业务”中Perf/W:>3X、Perf/$(BOM):>2X、软件后向兼容可靠。  
- [通用智能芯片设计公司“壁仞科技”首款7nm GPU芯片预计今年三季度流片，明年正式发布](https://mp.weixin.qq.com/s/nblHuDI8AXxjp__o4w46HA)  
摘要：壁仞科技CTO兼首席架构师洪洲表示，公司首款支持AI训练和推理的7nm芯片进展顺利，壁仞科技的第一款GPU芯片定位高端通用智能计算，具备高性能、可扩展性、可虚拟化等特性，支持云端训练和推理，目前已经到了收尾阶段，预计将在今年流片，**性能将与 NVIDIA 的下一代 GPU 相媲美，这颗芯片对标的，是国际GPU霸主英伟达还在酝酿之中的下一代5nm GPU计算芯片。**
壁仞科技的策略是，先聚焦几个点上，打一场“不对称的战争”。英伟达GPU并非面向AI训练和推理的最优芯片，而是一个多能力芯片。以A100为例，其双精度对HPC很重要，但对AI加速来说，其在能效比、算力等方面并非最优解。因此壁仞科技选择首先专攻通用AI训练和推理能力，将图形渲染等与AI加速无关的设计剥离掉，更聚焦于在自家芯片上如何合理安排更多的运算和存储单元。等芯片流片后，壁仞科技下一步将重点推进加速芯片商用落地的软件工作。  
**壁仞科技的第二款芯片已经开始启动架构设计，之后壁仞科技还将逐步推出面向智算中心、云游戏、边缘计算的GPU芯片。**  
壁仞科技着重优化其芯片的3个亮点特性：通用性、高算力、芯粒（chiplet）技术：  
    1. 通用性：从兼容CUDA到取代CUDA。新的GPU板卡要无缝地支持CUDA生态，这比更高的算力，更好的能效比更重要。壁仞科技的终极目标，是提供比CUDA更好的自研编程模型。  
    2. 高算力：融合多种架构的优点。以通用性为根本的同时，在专用领域做深耕、优化，融入多种架构的优点。不拘泥于传统的向量流处理架构，而会在其理念中加入数据流处理单元、近存储计算架构等其他元素，并对重点场景进行特殊优化，使其能处理各种数据类型，从而在同等能耗上，获得比英伟达高好几倍的算力。单颗芯片算力的提升只是一个点，壁仞科技还在其芯片中引入非常高的互连带宽，能做到数百数千的芯片大规模拓展，从而实现集群化大算力。  
    3. 芯粒（chiplet）：提高性价比的必备技术。  
截至目前，团队已扩张至超400人，并仍在续吸纳国内外人才。
- [抢先体验NVIDIA DOCA，走在技术前沿 | NVIDIA英伟达](https://mp.weixin.qq.com/s/AFyVF6sIzWMJHmVeRaGwQw)  
摘要：CPU 用于通用计算，GPU 用于加速计算，DPU 则进行数据处理。CPU、GPU、DPU 正在成为未来计算的三大支柱。  
NVIDIA BlueField DPU是一种新型可编程处理器，专注于数据处理，能够满足企业对性能、安全性、可管理等越来越高的需求，有高性能及软件可编程的多核CPU、高性能网络接口、灵活且可编程的加速引擎。为了加速数据中心的部署、支持广大开发者在BlueField DPU进行软件开发，NVIDIA还为DPU量身打造了一个软件开发套件 —— DOCA。  
- [三星新旗舰Exynos 2200性能前瞻：AMD GPU神助攻 | 电脑爱好者](https://mp.weixin.qq.com/s/_A4sOoioaWBR6xxGh4GdYw)  
摘要：高通明年初量产的骁龙898（或骁龙895），将采用ARMv9指令集定制Cortex-X2、Cortex-A710、Cortex-510架构魔改而来的Kryo 780核心，并集成Adreno 730以及全新的DSP和ISP单元，整体性能将有巨大的提升。  
骁龙898能否保持Android顶级SoC一哥的地位？有关三星下一代旗舰级SoC——Exynos 2200，三星准备了四款样片，其中2款采用Cortex-X1做超大核，1款采用最新的Cortex-X2做超大核，还有1款保守的型号则采用Cortex-A78。作为消费者，我们自然是希望Exynos 2200可以直接用上Cortex-X2+Cortex-A710+Cortex-510这种纯ARMv9架构打造的CPU矩阵。  
和其他竞品相比，三星Exynos 2200最大的特色，就是集成了来自AMD授权定制的RDNA 2架构GPU。前不久网上还传出了一张三星Exynos处理器运行3DMark Wild Life基准测试的成绩图，截图显示，带有AMD Radeon GPU和Cortex-A77 CPU的Exynos芯片组得分8134分，平均帧率为50FPS。  
从Cortex-A77 CPU架构来看，Exynos处理器应该属于还半成品，主要就是用于验证AMD Radeon GPU的可靠性与实力。当这颗GPU与Cortex-X2为代表的ARMv9架构核心搭配时，理论上可以释放出更强悍的性能。作为对比，高通骁龙888和麒麟9000的3DMark Wild Life基准测试跑分分别为5720和6677，由此可见AMD Radeon GPU的性能还是非常给力的，反正肯定是不会输给高通下一代的Adreno 730 GPU。  
- [瑞芯微Toybrick推出TB-RK3568X、TB-RV1126D开发板，支持海量应用场景 | 瑞芯微电子](https://mp.weixin.qq.com/s/l7qhxh-NU6tLJXs1FT1uIA)  
摘要：瑞芯微Toybrick正式发布全新TB-RK3568X和TB-RV1126D开发板，以多维技术优势支持海量场景。  
TB-RK3568X开发板具有五大产品特性，支持Android 11、Debian10、buildroot系统，主要面向智慧视觉、云终端、网络视频录像机 (NVR&XVR)、物联网网关、工控、网络附属存储 (NAS)、KTV点唱机等行业应用领域。  
1. 搭载高性能应用处理器：瑞芯微RK3568 CPU为四核Cortex-A55，采用全新Arm v8.2-A架构，效能有效提升；GPU为Mali-G52 ，双核心架构，图像API支持OpenGL ES 3.2, 2.0, 1.1，Vulkan 1.1。内置瑞芯微自研第三代独立NPU，可提供1T算力，支持Caffe/TensorFlow/TFLite/。ONNX/PyTorch/Keras/Darknet主流架构模型的一键转换。瑞芯微RV1126是专用于视觉处理的高性能处理器SoC，基于四核Arm Cortex A7 32位内核架构，集成NEON和FPU。每个核心都有一个32KB I cache和32KB D cache以及512KB的共用二级缓存。内置NPU，提供2T算力，支持 INT8/INT16混合操作，AI性能强大。RV1126具备强大的兼容性，支持主流网络模型如TensorFlow/MXNet/PyTorch/Caffe等轻松转换。  
2. 强大的编解码能力。支持4K@30fps+1080p@30fps视频编码、4K@30fps视频解码、H264/H265编解码等。  


## 论文


- [华为诺亚加法网络再升级：精度提升，可以逼近任意函数 | 机器之心](https://mp.weixin.qq.com/s/LzcGFlEwzMiSf-BEYIOFjg)  
摘要：深度卷积神经网络的计算常常需要巨大的能耗，因此难以在移动设备上实现。为此学界正在探索研究各式各样的新方法，本文要介绍的这项研究提出了使用加法替代 CNN 中的乘法（卷积），从而极大降低神经网络使用时的能耗。
众所周知，乘法的速度慢于加法，但是深度神经网络前向推理过程中的计算包含了大量权重和激活函数之间的乘法。因此，许多论文尝试研究了如何减少神经网络中的乘法计算，从而加快深度学习速度。  
在这一方向上的开创性研究是 Yoshua Bengio 团队 2015 年提出的 BinaryConnect，其会将网络权重强制设为二值（比如 -1 或 1，这个过程称为二值化），使得卷积网络中的乘加运算可被简单的累加运算替代。在那之后，该团队进一步提出了二值化神经网络（Binarized neural networks，BNN），其不仅会将权重变为二值，而且也会将卷积神经网络运行时的激活也设为二值。 
而在 AdderNet 中，特征往往聚集在不同的类别中心，这是因为 AdderNet 使用了 L1 距离来区别不同的类别。这个可视化结果说明 L1 距离和传统的互相关都可被用作度量，来计算深度神经网络中滤波器与输入特征之间距离的相似度。由于减法可使用加法的补码轻松实现，因此仅使用加法的 L1 度量可作为一种对硬件友好的度量，并且在构建神经网络时可以很自然地有效替代卷积。 
该研究的初步结果已在 CVPR 2020 发表（arXiv:1912.13200）。本文要介绍的是最新的研究成果，在新版本中，AdderNet 的性能已经获得了显著的提升，并且还具有了完善的理论保证。  
在图像分类任务上，AdderNet 的表现与 CNN 相近，且几乎不需要任何乘法计算，在精度上优于二值化神经网络（BNN，不过 BNN 的模型要小得多）。在常用的 ResNet-18 和 ResNet-50 架构上，AdderNet 新版本的精度已经和原始卷积网络十分相似，差距在一个点以内。  
- [2104.14129] [伯克利大学ActNN：节省显存新思路，在 PyTorch 里使用 2 bit 激活压缩训练神经网络 | 机器之心](https://mp.weixin.qq.com/s/hdulZ39EXSNnojQJ9HH4Tg)  
论文：https://arxiv.org/abs/2104.14129  
代码：https://github.com/ucbrise/actnn  
摘要：随着超大规模深度学习模型逐渐成为 AI 的趋势，如何在有限的 GPU 内存下训练这些模型成为了一个难题。本文将介绍来自加州伯克利大学的 ActNN，一个基于 PyTorch 的激活压缩训练框架。在同样的内存限制下，ActNN 通过使用 2 bit 激活压缩，可以将 batch size 扩大 6-14 倍，将模型尺寸或者输入图片扩大 6-10 倍。ActNN 相关论文已被 ICML 2021 接收为 Long Talk，代码开源于 github。  
目前，节省训练内存的方法主要有三类：1. 重计算（Gradient checkpointing/Rematerialization)  2. 使用 CPU 内存进行交换 (swapping)  和 3. 使用分布式训练将 Tensor 分散存储在多个 GPU 上。这三类方法互相不冲突，可以结合使用。大部分机器学习框架对这些方法都提供了一些支持，也有不少相关的论文。但是，想要高效、自动化地实现这些策略并不容易。与已有方法不同，作者提出了 ActNN，一个新的基于压缩的内存节省框架。在提供理论证明的同时基于 PyTorch 提供了一个高效易用的实现：ActNN 可以将 batch size 扩大 6-14 倍，将模型尺寸或者输入图片扩大 6-10 倍。  



## 开源项目


> 注：每条内容前缀为github地址的仓库拥有者和仓库名，补全地址后为`github.com/<repo_owner>/<repo_name>`。

- [microsoft/SuperBench：Hardware and Software Benchmarks for AI Systems](https://github.com/microsoft/superbenchmark)  
项目：https://microsoft.github.io/superbenchmark/  
摘要：SuperBench is a validation and profiling tool for AI infrastructure. It provides micro-benchmark for primitive computation and communication benchmarking, as well as model-benchmark to measure domain-aware end-to-end deep learning workloads. SuperBench supports:
    - AI infrastructure validation and diagnosis
        - Distributed validation tools to validate hundreds or thousands of servers automatically
        - Consider both raw hardware and E2E model performance with ML workload patterns
        - Build a contract to identify hardware issues
        - Provide infrastructural-oriented criteria as Performance/Quality Gates for hardware and system release
        - Provide detailed performance report and advanced analysis tool
    - AI workload benchmarking and profiling
        - Provide comprehensive performance comparison between different existing hardware
        - Provide insights for hardware and software co-design

## 博文


- [『豪门盛宴』AI训练芯片和系统最新战况 | StarryHeavensAbove](https://mp.weixin.qq.com/s/jVEY3PFeFymyKCL0oz6D8A)  
摘要：MLPerf的工作已经持续了几年，Training的Benchmark是最早开展工作，也是到目前为止最受瞩目的工作。不出意料的话，各个厂商都会“宣称”自己夺得了“xxx的冠军”。0.7版本的结果出来之后，Nvidia做了一个对比图，试图做归一化的比较，但也很难说是非常合理的对比。我之前对这个问题的比喻是“关公战秦琼”，这也是MLPerf基准测试面临的困难之一。
那么，如果结果很难对比的话，组织和参与这样的“竞赛”的意义是什么呢？  
第一，这符合“AI训练军备竞赛”的需要。只要在AI上“大力出奇迹”的规律不变，AI模型的规模就会越来越大。  
第二，有利于提升芯片厂商的综合能力。如前面MLPerf的简单介绍，它测试的是软硬件系统的综合能力。如果我们观察华为ResNet的数据，0.7版本使用Tensoflow框架，这次使用的是Mindspore。  
第三，Benchmark的设计和讨论本身也可以促进相关技术的进步。设计一个好的Benchmark也是高技术工作，需要对算法和系统软硬件的技术趋势和实现有深入的理解。  
- [技术解码 | 变局时代：RISC-V处理器架构的技术演变及商业想象 | 芯片开放社区](https://mp.weixin.qq.com/s/6U34aBHzlleeqKhelpZfYw)  
摘要：RISC-V架构近几年的迅猛发展，使其在学术界和产业界的热度不断提升。凭借极简、模块化、可拓展等属性，RISC-V有望成为新时代的主导架构。从本期开始，我们将为大家推荐RISC-V知识图谱系列内容，带大家从“应用开发”、“商业”、“技术创新”等不同角度视角认识RISC-V。本文为RISC-V知识图谱系列的开篇之作，主要从技术与商业视角介绍RISC-V架构的起源，分析RISC-V发展如此迅速的原因，并剖析X86架构与ARM架构成功的原因。  
ARM成功的主要原因，就是把处理器架构作为一种基础技术被其他的芯片厂商所集成，从而将一堆的芯片公司围绕在自己身边并形成了自己的小伙伴阵营。当然，ARM的成功也得益于历史的大趋势，那就是功能手机的出现，以及功能手机之后智能手机的兴起和普及，商业模式的革命遇上历史的浪潮使得ARM从传统的农村地带逐步进入了城市地带并最终进入了舞台的中央（ARM目前已经杀入了X86的传统强势地盘，包括苹果的M1 PC机芯片以及Ampere/亚马逊等的服务器芯片）。  
